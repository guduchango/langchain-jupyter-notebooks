{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from IPython.display import HTML\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 1: Configuration\n",
    "# ------------------------------------------------------------\n",
    "# We define the paths where our documents and vector database will live.\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "markdown_dir = \"../data/external\"      # Folder with markdown files\n",
    "persist_dir = \"../data/chroma_db/01\"   # Where the Chroma database will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b3c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# STEP 2: Load documents\n",
    "# ------------------------------------------------------------\n",
    "# We go through all markdown files and load them using TextLoader.\n",
    "# Each document gets metadata like its filename and a unique SHA256 hash.\n",
    "raw_documents = []\n",
    "for filename in os.listdir(markdown_dir):\n",
    "    if filename.endswith(\".md\"):\n",
    "        filepath = os.path.join(markdown_dir, filename)\n",
    "        loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = filename\n",
    "            doc.metadata[\"hash\"] = hashlib.sha256(doc.page_content.encode(\"utf-8\")).hexdigest()\n",
    "            raw_documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef94b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Total number of chunks: 104\n",
      "\n",
      "Chunk 1: # Varonil - Signature Tailoring  **Motto:** \"Style is not bought, it is cultivated. Quality is not negotiated, it is dem... (Source: Varonil.md)\n",
      "Chunk 2: We believe in a world where one buys less, but chooses better. Each of our pieces is designed to last for generations, n... (Source: Varonil.md)\n",
      "Chunk 3: - **VicuÃ±a Wool from the Puna:** Considered the finest and rarest animal fiber in the world. It is obtained by harmlessl... (Source: Varonil.md)\n",
      "Chunk 4: - **Pima Cotton from the CalchaquÃ­ Valleys:** Hand-harvested in Salta, this extra-long staple cotton ensures exceptional... (Source: Varonil.md)\n",
      "Chunk 5: ## 3. The \"Estirpe\" Permanent Collection  We do not produce seasonal collections. We offer a set of iconic pieces, perfe... (Source: Varonil.md)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# STEP 3: Split documents into chunks\n",
    "# ------------------------------------------------------------\n",
    "# Large documents are split into smaller fragments (chunks).\n",
    "# This makes it easier for the AI to retrieve precise answers.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=75)\n",
    "split_documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "print(f\"ðŸ“„ Total number of chunks: {len(split_documents)}\\n\")\n",
    "\n",
    "# Let's show a quick preview of the first few chunks.\n",
    "for i, doc in enumerate(split_documents[:5], start=1):\n",
    "    preview = doc.page_content[:120].replace(\"\\n\", \" \") + (\"...\" if len(doc.page_content) > 120 else \"\")\n",
    "    print(f\"Chunk {i}: {preview} (Source: {doc.metadata.get('source', 'unknown')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6df0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Create embeddings\n",
    "# ------------------------------------------------------------\n",
    "# Here we transform each text chunk into a numerical vector (embedding).\n",
    "# We use HuggingFace's MiniLM model because it is small, fast, and effective.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f2a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Creating new vector store...\n",
      "ðŸ“¦ Number of documents in the store: 104\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Create or load the Chroma vector database\n",
    "# ------------------------------------------------------------\n",
    "# If the database doesn't exist, we build it from our split documents.\n",
    "# Otherwise, we simply load the existing database.\n",
    "os.makedirs(persist_dir, exist_ok=True)\n",
    "if len(os.listdir(persist_dir)) == 0:\n",
    "    print(\"âš¡ Creating new vector store...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=split_documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "else:\n",
    "    print(\"âœ… Loading existing vector store...\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "# Show how many documents are currently in the database.\n",
    "print(\"ðŸ“¦ Number of documents in the store:\", vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358fc8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Preview of stored data (text + embedding numbers):\n",
      "\n",
      "--- Document 1 ---\n",
      "Text preview: # Varonil - Signature Tailoring  **Motto:** \"Style is not bought, it is cultivated. Quality is not n...\n",
      "Embedding (first 10 values): [-0.0507, -0.0285, -0.2047, -0.0649, -0.1940, 0.1361, 0.1589, 0.1468, -0.0748, -0.1889]\n",
      "(Source: Varonil.md)\n",
      "\n",
      "--- Document 2 ---\n",
      "Text preview: We believe in a world where one buys less, but chooses better. Each of our pieces is designed to las...\n",
      "Embedding (first 10 values): [-0.0294, -0.0250, -0.0426, -0.1478, 0.0665, -0.2179, -0.0942, 0.0094, -0.0485, -0.0820]\n",
      "(Source: Varonil.md)\n",
      "\n",
      "--- Document 3 ---\n",
      "Text preview: - **VicuÃ±a Wool from the Puna:** Considered the finest and rarest animal fiber in the world. It is o...\n",
      "Embedding (first 10 values): [0.0258, 0.1227, 0.0576, -0.1084, 0.0754, -0.2187, 0.1298, -0.0948, -0.1254, -0.0063]\n",
      "(Source: Varonil.md)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Preview stored data and embeddings\n",
    "# ------------------------------------------------------------\n",
    "# We can retrieve documents and embeddings to see how text has been transformed into numbers.\n",
    "sample_data = vectorstore.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n",
    "\n",
    "print(\"\\nðŸ” Preview of stored data (text + embedding numbers):\\n\")\n",
    "for i, (doc, emb, meta) in enumerate(\n",
    "    zip(sample_data[\"documents\"][:3], sample_data[\"embeddings\"][:3], sample_data[\"metadatas\"][:3]),\n",
    "    start=1\n",
    "):\n",
    "    # Show a snippet of the text\n",
    "    preview_text = doc[:100].replace(\"\\n\", \" \") + (\"...\" if len(doc) > 100 else \"\")\n",
    "    # Show the first 10 numbers of the embedding vector\n",
    "    preview_embedding = \", \".join(f\"{x:.4f}\" for x in emb[:10])\n",
    "\n",
    "    print(f\"--- Document {i} ---\")\n",
    "    print(f\"Text preview: {preview_text}\")\n",
    "    print(f\"Embedding (first 10 values): [{preview_embedding}]\")\n",
    "    print(f\"(Source: {meta.get('source', 'unknown')})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c70955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Define a prompt template\n",
    "# ------------------------------------------------------------\n",
    "# This template will be used to structure the question and the context\n",
    "template = \"\"\"\n",
    "You are an assistant that answers questions based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a clear and concise way:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a227c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Load a local LLM with Ollama\n",
    "# ------------------------------------------------------------\n",
    "# We use a light model (like mistral) that runs on local hardware.\n",
    "llm = OllamaLLM(model=\"phi3:mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3e6413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Build a RetrievalQA chain\n",
    "# ------------------------------------------------------------\n",
    "# The retriever will find relevant chunks, and the LLM will use them to answer.\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¾ Prompt sent to the model:\n",
      "\n",
      "You are an assistant that answers questions based on the provided context.\n",
      "\n",
      "Context:\n",
      "## 4. The Capsule Collection\n",
      "\n",
      "### 4.1. The Catalyst Blazer\n",
      "\n",
      "The cornerstone of a power wardrobe. Structured, but as comfortable as your favorite cardigan.\n",
      "- **Material:** Signature **Flex-Formâ„¢** Twill.\n",
      "- **Features:** Action-Back Construction, Secure-Zip Pockets (2 external, 1 internal), single-button closure.\n",
      "- **Fit:** Tailored, with a slightly nipped-in waist.\n",
      "- **Colors:** Onyx Black, Midnight Navy, Slate Gray, Ivory.\n",
      "- **Price:** $295 USD.\n",
      "\n",
      "### 4.2. The Pivot Pant\n",
      "## 5. Frequently Asked Questions (FAQ)\n",
      "\n",
      "- **Q: Why is the \"Legado\" Blazer so expensive?**\n",
      "  - A: The price reflects the use of vicuÃ±a wool, one of the rarest and most costly fibers in the world, and the 80 hours of artisanal labor each piece requires. It is an investment in a functional work of art.\n",
      "## 6. Frequently Asked Questions (FAQ)\n",
      "\n",
      "- **Q: Are the pockets on the Catalyst Blazer actually real and functional?**\n",
      "  - A: Yes, absolutely. They are a core feature of our design philosophy. The two external pockets are deep enough for a large phone or notebook, and the internal pocket is perfect for a passport or cards.\n",
      "We believe in a world where one buys less, but chooses better. Each of our pieces is designed to last for generations, not seasons. We do not follow trends; we forge a legacy of timeless elegance.\n",
      "\n",
      "## 2. Noble Materials: The Soul of the Garment\n",
      "\n",
      "Our quest for excellence begins at the source. We work exclusively with ethically sourced materials of superlative quality.\n",
      "\n",
      "Question:\n",
      "What is the price of The Catalyst Blazer?\n",
      "\n",
      "Answer in a clear and concise way:\n",
      "\n",
      "\n",
      "ðŸ¤– Model response:\n",
      "\n",
      "The Catalyst Blazer is priced at $295 USD.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Ask a question and inspect the prompt\n",
    "# ------------------------------------------------------------\n",
    "query = \"What is the price of The Catalyst Blazer?\"\n",
    "\n",
    "# Manually retrieve the relevant documents (chunks)\n",
    "docs = vectorstore.as_retriever(search_kwargs={\"k\": 4}).get_relevant_documents(query)\n",
    "\n",
    "# Build the context from the retrieved chunks\n",
    "context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Fill the prompt template with actual context and question\n",
    "filled_prompt = prompt.format(context=context, question=query)\n",
    "\n",
    "# Show the exact prompt sent to the LLM\n",
    "print(\"\\nðŸ§¾ Prompt sent to the model:\")\n",
    "print(filled_prompt)\n",
    "\n",
    "# Directly invoke the model with the composed prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "# Display the model's response\n",
    "print(\"\\nðŸ¤– Model response:\\n\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
