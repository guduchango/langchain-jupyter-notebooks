{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from IPython.display import HTML\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 1: Configuration\n",
    "# ------------------------------------------------------------\n",
    "# We define the paths where our documents and vector database will live.\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "markdown_dir = \"../data/external\"      # Folder with markdown files\n",
    "persist_dir = \"../data/chroma_db/01\"   # Where the Chroma database will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b3c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# STEP 2: Load documents\n",
    "# ------------------------------------------------------------\n",
    "# We go through all markdown files and load them using TextLoader.\n",
    "# Each document gets metadata like its filename and a unique SHA256 hash.\n",
    "raw_documents = []\n",
    "for filename in os.listdir(markdown_dir):\n",
    "    if filename.endswith(\".md\"):\n",
    "        filepath = os.path.join(markdown_dir, filename)\n",
    "        loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = filename\n",
    "            doc.metadata[\"hash\"] = hashlib.sha256(doc.page_content.encode(\"utf-8\")).hexdigest()\n",
    "            raw_documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef94b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Total number of chunks: 104\n",
      "\n",
      "Chunk 1: # Varonil - Signature Tailoring  **Motto:** \"Style is not bought, it is cultivated. Quality is not negotiated, it is dem... (Source: Varonil.md)\n",
      "Chunk 2: We believe in a world where one buys less, but chooses better. Each of our pieces is designed to last for generations, n... (Source: Varonil.md)\n",
      "Chunk 3: - **Vicuña Wool from the Puna:** Considered the finest and rarest animal fiber in the world. It is obtained by harmlessl... (Source: Varonil.md)\n",
      "Chunk 4: - **Pima Cotton from the Calchaquí Valleys:** Hand-harvested in Salta, this extra-long staple cotton ensures exceptional... (Source: Varonil.md)\n",
      "Chunk 5: ## 3. The \"Estirpe\" Permanent Collection  We do not produce seasonal collections. We offer a set of iconic pieces, perfe... (Source: Varonil.md)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# STEP 3: Split documents into chunks\n",
    "# ------------------------------------------------------------\n",
    "# Large documents are split into smaller fragments (chunks).\n",
    "# This makes it easier for the AI to retrieve precise answers.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=75)\n",
    "split_documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "print(f\"📄 Total number of chunks: {len(split_documents)}\\n\")\n",
    "\n",
    "# Let's show a quick preview of the first few chunks.\n",
    "for i, doc in enumerate(split_documents[:5], start=1):\n",
    "    preview = doc.page_content[:120].replace(\"\\n\", \" \") + (\"...\" if len(doc.page_content) > 120 else \"\")\n",
    "    print(f\"Chunk {i}: {preview} (Source: {doc.metadata.get('source', 'unknown')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6df0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Create embeddings\n",
    "# ------------------------------------------------------------\n",
    "# Here we transform each text chunk into a numerical vector (embedding).\n",
    "# We use HuggingFace's MiniLM model because it is small, fast, and effective.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f2a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Creating new vector store...\n",
      "📦 Number of documents in the store: 104\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Create or load the Chroma vector database\n",
    "# ------------------------------------------------------------\n",
    "# If the database doesn't exist, we build it from our split documents.\n",
    "# Otherwise, we simply load the existing database.\n",
    "os.makedirs(persist_dir, exist_ok=True)\n",
    "if len(os.listdir(persist_dir)) == 0:\n",
    "    print(\"⚡ Creating new vector store...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=split_documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "else:\n",
    "    print(\"✅ Loading existing vector store...\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "# Show how many documents are currently in the database.\n",
    "print(\"📦 Number of documents in the store:\", vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358fc8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Preview of stored data (text + embedding numbers):\n",
      "\n",
      "--- Document 1 ---\n",
      "Text preview: # Varonil - Signature Tailoring  **Motto:** \"Style is not bought, it is cultivated. Quality is not n...\n",
      "Embedding (first 10 values): [-0.0507, -0.0285, -0.2047, -0.0649, -0.1940, 0.1361, 0.1589, 0.1468, -0.0748, -0.1889]\n",
      "(Source: Varonil.md)\n",
      "\n",
      "--- Document 2 ---\n",
      "Text preview: We believe in a world where one buys less, but chooses better. Each of our pieces is designed to las...\n",
      "Embedding (first 10 values): [-0.0294, -0.0250, -0.0426, -0.1478, 0.0665, -0.2179, -0.0942, 0.0094, -0.0485, -0.0820]\n",
      "(Source: Varonil.md)\n",
      "\n",
      "--- Document 3 ---\n",
      "Text preview: - **Vicuña Wool from the Puna:** Considered the finest and rarest animal fiber in the world. It is o...\n",
      "Embedding (first 10 values): [0.0258, 0.1227, 0.0576, -0.1084, 0.0754, -0.2187, 0.1298, -0.0948, -0.1254, -0.0063]\n",
      "(Source: Varonil.md)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Preview stored data and embeddings\n",
    "# ------------------------------------------------------------\n",
    "# We can retrieve documents and embeddings to see how text has been transformed into numbers.\n",
    "sample_data = vectorstore.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n",
    "\n",
    "print(\"\\n🔍 Preview of stored data (text + embedding numbers):\\n\")\n",
    "for i, (doc, emb, meta) in enumerate(\n",
    "    zip(sample_data[\"documents\"][:3], sample_data[\"embeddings\"][:3], sample_data[\"metadatas\"][:3]),\n",
    "    start=1\n",
    "):\n",
    "    # Show a snippet of the text\n",
    "    preview_text = doc[:100].replace(\"\\n\", \" \") + (\"...\" if len(doc) > 100 else \"\")\n",
    "    # Show the first 10 numbers of the embedding vector\n",
    "    preview_embedding = \", \".join(f\"{x:.4f}\" for x in emb[:10])\n",
    "\n",
    "    print(f\"--- Document {i} ---\")\n",
    "    print(f\"Text preview: {preview_text}\")\n",
    "    print(f\"Embedding (first 10 values): [{preview_embedding}]\")\n",
    "    print(f\"(Source: {meta.get('source', 'unknown')})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c70955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Define a prompt template\n",
    "# ------------------------------------------------------------\n",
    "# This template will be used to structure the question and the context\n",
    "template = \"\"\"\n",
    "You are an assistant that answers questions based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a clear and concise way:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a227c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Load a local LLM with Ollama\n",
    "# ------------------------------------------------------------\n",
    "# We use a light model (like mistral) that runs on local hardware.\n",
    "llm = OllamaLLM(model=\"phi3:mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3e6413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Build a RetrievalQA chain\n",
    "# ------------------------------------------------------------\n",
    "# The retriever will find relevant chunks, and the LLM will use them to answer.\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧾 Prompt sent to the model:\n",
      "\n",
      "You are an assistant that answers questions based on the provided context.\n",
      "\n",
      "Context:\n",
      "## 4. The Capsule Collection\n",
      "\n",
      "### 4.1. The Catalyst Blazer\n",
      "\n",
      "The cornerstone of a power wardrobe. Structured, but as comfortable as your favorite cardigan.\n",
      "- **Material:** Signature **Flex-Form™** Twill.\n",
      "- **Features:** Action-Back Construction, Secure-Zip Pockets (2 external, 1 internal), single-button closure.\n",
      "- **Fit:** Tailored, with a slightly nipped-in waist.\n",
      "- **Colors:** Onyx Black, Midnight Navy, Slate Gray, Ivory.\n",
      "- **Price:** $295 USD.\n",
      "\n",
      "### 4.2. The Pivot Pant\n",
      "## 5. Frequently Asked Questions (FAQ)\n",
      "\n",
      "- **Q: Why is the \"Legado\" Blazer so expensive?**\n",
      "  - A: The price reflects the use of vicuña wool, one of the rarest and most costly fibers in the world, and the 80 hours of artisanal labor each piece requires. It is an investment in a functional work of art.\n",
      "## 6. Frequently Asked Questions (FAQ)\n",
      "\n",
      "- **Q: Are the pockets on the Catalyst Blazer actually real and functional?**\n",
      "  - A: Yes, absolutely. They are a core feature of our design philosophy. The two external pockets are deep enough for a large phone or notebook, and the internal pocket is perfect for a passport or cards.\n",
      "We believe in a world where one buys less, but chooses better. Each of our pieces is designed to last for generations, not seasons. We do not follow trends; we forge a legacy of timeless elegance.\n",
      "\n",
      "## 2. Noble Materials: The Soul of the Garment\n",
      "\n",
      "Our quest for excellence begins at the source. We work exclusively with ethically sourced materials of superlative quality.\n",
      "\n",
      "Question:\n",
      "What is the price of The Catalyst Blazer?\n",
      "\n",
      "Answer in a clear and concise way:\n",
      "\n",
      "\n",
      "🤖 Model response:\n",
      "\n",
      "The Catalyst Blazer is priced at $295 USD.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Ask a question and inspect the prompt\n",
    "# ------------------------------------------------------------\n",
    "query = \"What is the price of The Catalyst Blazer?\"\n",
    "\n",
    "# Manually retrieve the relevant documents (chunks)\n",
    "docs = vectorstore.as_retriever(search_kwargs={\"k\": 4}).get_relevant_documents(query)\n",
    "\n",
    "# Build the context from the retrieved chunks\n",
    "context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Fill the prompt template with actual context and question\n",
    "filled_prompt = prompt.format(context=context, question=query)\n",
    "\n",
    "# Show the exact prompt sent to the LLM\n",
    "print(\"\\n🧾 Prompt sent to the model:\")\n",
    "print(filled_prompt)\n",
    "\n",
    "# Directly invoke the model with the composed prompt\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "# Display the model's response\n",
    "print(\"\\n🤖 Model response:\\n\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
